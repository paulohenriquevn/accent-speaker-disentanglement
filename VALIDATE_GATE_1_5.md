### ğŸ” Projetos e recursos abertos relevantes

**S3PRL-VC** â€” um framework open-source para voice conversion que usa representaÃ§Ãµes auto-supervisionadas e explora *disentanglement* de caracterÃ­sticas de fala. Ã‰ Ãºtil como baseline para testar representaÃ§Ãµes de sotaque/identidade antes de um LoRA. ([ResearchGate][1])

**Retrieval-based Voice Conversion (RVC)** â€” um projeto open-source de *voice conversion* no GitHub que preserva atributos vocais e pode servir como ponto de partida para experimentar separabilidade em transformaÃ§Ãµes de voz. ([Wikipedia][2])

**ASLP-labâ€™s MeanVC e VoiceSculptor** â€” projetos sob a organizaÃ§Ã£o ASLP-lab que tÃªm cÃ³digo pÃºblico para tarefas de manipulaÃ§Ã£o e conversÃ£o de voz; *MeanVC* em particular Ã© pensado para conversÃ£o zero-shot e pode ser um bom recurso de implementaÃ§Ã£o prÃ³pria de separabilidade latente. ([GitHub][3])

**ESPnet / ESPnet-TTS** â€” nÃ£o focado especificamente em disentanglement, mas um toolkit TTS open-source extensÃ­vel, que facilita experimentos com representaÃ§Ãµes internas e anÃ¡lises de feature spaces. ([arXiv][4])

**Corpora e ferramentas de fala**
Projetos como *falabrasil/speech-datasets* oferecem bases de Ã¡udio transcrito em PortuguÃªs Brasileiro, alinhadas com ferramentas como Kaldi para anÃ¡lise acÃºstica â€” isso Ã© Ãºtil para criar splits controlados e features de sotaque. ([GitHub][5])

### ğŸ“š Artigos e pesquisa acadÃªmica

**SpeechSplit / SpeechSplit 2.0** â€” uma linha clÃ¡ssica de trabalhos que explora *disentanglement* de conteÃºdo, pitch, ritmo e timbre em representaÃ§Ãµes de fala. Mesmo anterior a 2026, essa literatura ainda Ã© um bom ponto de partida para o seu protocolo de anÃ¡lise latente. ([arXiv][6])

**Accent-VITS** â€” pesquisa recente sobre transferÃªncia de sotaque em TTS que explicitamente trata de separaÃ§Ã£o de timbre e sotaque usando variÃ¡veis latentes hierÃ¡rquicas. Esse tipo de abordagem tem muita relevÃ¢ncia para avaliar disentanglement em backbone antes de LoRA. ([arXiv][7])

**ParaMETA (2026)** â€” trabalho muito recente listado em agregadores de papers que aborda *disentangled paralinguistic style*, ou seja, representa estilos de fala de forma separÃ¡vel no contexto de grandes modelos de fala. Esse tipo de artigo estÃ¡ alinhado com a ideia de medir separabilidade latente. ([GitHub][8])

**LLASO (ICLR 2026)** â€” ainda sob revisÃ£o, mas um exemplo de esforÃ§o em criar bases, benchmarks e modelos abertos para speech + language com foco em reprodutibilidade â€” Ãºtil se vocÃª quer uma baseline pÃºblica e padronizada para comparar representaÃ§Ãµes latentes. ([OpenReview][9])

---

### ğŸ§  Como isso se encaixa no seu Stage 1.5

Essas ferramentas/artigos nÃ£o resolvem completa e magicamente o problema de separabilidade de sotaque versus identidade, mas elas dÃ£o **modelos, frameworks e benchmarks nos quais vocÃª pode apoiar a sua anÃ¡lise latente**:

* S3PRL-VC e RVC dÃ£o implementaÃ§Ãµes **prontas para voice conversion** que jÃ¡ tentam separar conteÃºdo e estilo, sendo um bom baseline para ver se um backbone tem alguma separabilidade natural. ([ResearchGate][1])

* Voice cloning architectures descritas em surveys de 2026 explicam tendÃªncias de decomposiÃ§Ã£o de embeddings em *linguistic*, *speaker* e *style*, que vocÃª pode usar como referÃªncia para desenho de probes. ([Emergent Mind][10])

* Artigos como SpeechSplit e Accent-VITS te dÃ£o **modelos e mÃ©tricas** de disentanglement que jÃ¡ foram validados em benchmarks pÃºblicos, ideais para comparar contra seus prÃ³prios probes. ([arXiv][6])

* Os agregadores de papers mostram que a comunidade, em 2026, estÃ¡ investindo bastante em **representaÃ§Ãµes latentes e estilos paralinguÃ­sticos**, entÃ£o vocÃª estÃ¡ na trilha certa. ([GitHub][8])

---

### ğŸ§© Onde comeÃ§ar

1. Clone **S3PRL-VC** e tente extrair features internas de um backbone como WavLM ou HuBERT e veja se accent/speaker sÃ£o separÃ¡veis por probes lineares. ([ResearchGate][1])
2. Experimente **RVC** como baseline de conversÃ£o e veja como o modelo representa sotaque e timbre em embeddings. ([Wikipedia][2])
3. Use **ESPnet** para gerar representaÃ§Ãµes e comparar com suas features internas. ([arXiv][4])
4. Leia e utilize mÃ©tricas e arquiteturas de SpeechSplit/Accent-VITS para estruturar seu que esses modelos realmente disentangle sotaque e identidade de forma robusta. ([arXiv][6])

---

Se quiser, posso te preparar **um roteiro de experimentos concretos** utilizando esses projetos (incluindo scripts e mÃ©tricas) para validar separabilidade de sotaque e identidade em um backbone especÃ­fico como Qwen3-TTS.

[1]: https://www.researchgate.net/publication/360792818_S3PRL-VC_Open-Source_Voice_Conversion_Framework_with_Self-Supervised_Speech_Representations?utm_source=chatgpt.com "S3PRL-VC: Open-Source Voice Conversion Framework ..."
[2]: https://en.wikipedia.org/wiki/Retrieval-based_Voice_Conversion?utm_source=chatgpt.com "Retrieval-based Voice Conversion"
[3]: https://github.com/ASLP-lab?utm_source=chatgpt.com "ASLP-lab"
[4]: https://arxiv.org/abs/1910.10909?utm_source=chatgpt.com "ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit"
[5]: https://github.com/falabrasil/speech-datasets?utm_source=chatgpt.com "falabrasil/speech-datasets: ğŸ—£ï¸ğŸ‡§ğŸ‡· Bases de Ã¡udio ..."
[6]: https://arxiv.org/abs/2203.14156?utm_source=chatgpt.com "SpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlenecks"
[7]: https://arxiv.org/abs/2312.16850?utm_source=chatgpt.com "Accent-VITS:accent transfer for end-to-end TTS"
[8]: https://github.com/halsay/ASR-TTS-paper-daily?utm_source=chatgpt.com "halsay/ASR-TTS-paper-daily: Update ASR paper everyday"
[9]: https://openreview.net/pdf/04d80f00e38671c90c1a5c2913bcd54bd1577e32.pdf?utm_source=chatgpt.com "LLASO"
[10]: https://www.emergentmind.com/topics/voice-cloning-models?utm_source=chatgpt.com "Voice Cloning Models Overview"
