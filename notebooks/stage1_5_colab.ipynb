{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1.5 — Colab Runner\n",
    "This notebook automates the Stage 1.5 latent separability audit (Accent × Speaker) inside Google Colab.\n",
    "\n",
    "**Pipeline overview**\n",
    "1. (Optional) Mount Google Drive to access private datasets/checkpoints.\n",
    "2. Clone this repository (or pull from your fork).\n",
    "3. Install dependencies with `pip install -e .[dev]`.\n",
    "4. Ensure `data/manifest.jsonl` and referenced audio files exist.\n",
    "5. Run the feature extractors (acoustic, ECAPA, SSL, backbone).\n",
    "6. Execute `stage1_5 run` to train probes, compute leakage/RSA/CKA, and render the GO/NOGO report.\n",
    "7. Download artifacts (`artifacts/analysis`, `report/`) or sync back to Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Runtime diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi || echo 'GPU not available (OK for CPU-only runs)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Optional) Mount Google Drive\n",
    "If your dataset or checkpoints live on Drive, mount it now. Skip if you plan to upload files manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "MOUNT_DRIVE = False  # set to True if you want to mount Drive\n",
    "if MOUNT_DRIVE:\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Repository + dataset configuration\n",
    "Set the repository URL/branch you want to run. Update paths if your manifest or audio live elsewhere (e.g., in Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = 'https://github.com/<your-org>/<repo>.git'  # TODO: update\n",
    "BRANCH = 'main'                                        # e.g., 'main' or 'stage1_5'\n",
    "WORKDIR = Path('/content/stage1_5')\n",
    "DATA_ROOT = WORKDIR / 'data'                           # adjust if mounting from Drive\n",
    "MANIFEST_PATH = DATA_ROOT / 'manifest.jsonl'\n",
    "CONFIG_PATH = Path('config/stage1_5.yaml')             # relative to WORKDIR\n",
    "\n",
    "print('Repo:', REPO_URL)\n",
    "print('Branch:', BRANCH)\n",
    "print('Working dir:', WORKDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clone / refresh the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "if WORKDIR.exists():\n",
    "    shutil.rmtree(WORKDIR)\n",
    "!git clone -b $BRANCH $REPO_URL $WORKDIR\n",
    "%cd $WORKDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pip\n",
    "!pip install -q -e .[dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset preparation\n",
    "Refer to `docs/dataset_guidelines.md` for the full specification.\n",
    "\n",
    "Options:\n",
    "- **Auto**: provide a dataset archive URL + metadata CSV and let the CLI download/extract/build the manifest.\n",
    "- **Manual**: upload your own `data/wav` + `data/manifest.jsonl` (skip the auto cells).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-config"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_URL = ''  # e.g., https://drive.google.com/uc?id=...\n",
    "DATASET_ARCHIVE_NAME = 'stage1_5_dataset.zip'\n",
    "DATASET_EXTRACT_DIR = Path('data/external')\n",
    "DATASET_METADATA_CSV = DATASET_EXTRACT_DIR / 'metadata.csv'\n",
    "DATASET_AUDIO_SUBDIR = DATASET_EXTRACT_DIR / 'wav'\n",
    "COPY_AUDIO_TO = DATA_ROOT / 'wav'\n",
    "AUTO_BUILD_MANIFEST = False  # set True to run CLI steps automatically\n",
    "print('Dataset extract dir:', DATASET_EXTRACT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-download"
   },
   "outputs": [],
   "source": [
    "if DATASET_URL:\n",
    "    !stage1_5 dataset download --url $DATASET_URL --output-dir $DATASET_EXTRACT_DIR --filename $DATASET_ARCHIVE_NAME\n",
    "else:\n",
    "    print('Set DATASET_URL to enable automatic download or skip to manual upload.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-build"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if AUTO_BUILD_MANIFEST and DATASET_AUDIO_SUBDIR.exists():\n",
    "    COPY_AUDIO_TO.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(DATASET_AUDIO_SUBDIR, COPY_AUDIO_TO, dirs_exist_ok=True)\n",
    "    if DATASET_METADATA_CSV.exists():\n",
    "        !stage1_5 dataset build-manifest $DATASET_METADATA_CSV --audio-root $COPY_AUDIO_TO --output $MANIFEST_PATH --source real\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Metadata CSV not found: {DATASET_METADATA_CSV}')\n",
    "else:\n",
    "    print('AUTO_BUILD_MANIFEST disabled or dataset folders missing; ensure data/manifest.jsonl exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset"
   },
   "outputs": [],
   "source": [
    "# Manual fallback: copy dataset into data/ then ensure manifest exists\n",
    "# Example: copy dataset from Drive\n",
    "# !cp -r /content/drive/MyDrive/stage1_5_data/* $DATA_ROOT\n",
    "\n",
    "if not MANIFEST_PATH.exists():\n",
    "    raise FileNotFoundError(f'Manifest not found: {MANIFEST_PATH}. Provide metadata or enable AUTO_BUILD_MANIFEST.')\n",
    "\n",
    "print('Manifest entries preview:')\n",
    "!head -n 5 $MANIFEST_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature extraction\n",
    "Uncomment the commands you need. You may run them separately to reuse cached features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "features"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Acoustic features PASS\n",
    "#stage1_5 features acoustic data/manifest.jsonl artifacts/features/acoustic\n",
    "\n",
    "# ECAPA embeddings (set device to 'cuda' if GPU is available) PASS\n",
    "# stage1_5 features ecapa data/manifest.jsonl artifacts/features/ecapa --device cuda\n",
    "\n",
    "# SSL features (HuBERT/WavLM via s3prl) FAILED\n",
    "stage1_5 features ssl data/manifest.jsonl artifacts/features/ssl --model wavlm_large\n",
    "\n",
    "# Backbone hooks (requires synthetic manifest + text prompts)\n",
    "# stage1_5 features backbone gen/manifest_syn.jsonl data/texts.json artifacts/features/backbone \\\n",
    "#     --checkpoint your-org/tts-backbone --layers encoder_out block_08 decoder_pre_vocoder\n",
    "\n",
    "# ECAPA embeddings (set device to 'cuda' if GPU is available)\n",
    "# stage1_5 features ecapa data/manifest.jsonl artifacts/features/ecapa --device cuda\n",
    "\n",
    "# SSL features (HuBERT/WavLM via s3prl)\n",
    "# stage1_5 features ssl data/manifest.jsonl artifacts/features/ssl --model wavlm_large\n",
    "\n",
    "# Backbone hooks (requires synthetic manifest + text prompts)\n",
    "# stage1_5 features backbone gen/manifest_syn.jsonl data/texts.json artifacts/features/backbone \\\n",
    "#     --checkpoint your-org/tts-backbone --layers encoder_out block_08 decoder_pre_vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Stage 1.5 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run"
   },
   "outputs": [],
   "source": [
    "!stage1_5 run $CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inspect metrics & figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.read_csv('artifacts/analysis/metrics.csv')\n",
    "metrics.sort_values('accent_f1', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "figures"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image('artifacts/analysis/figures/accent_f1.png'))\n",
    "display(Image('artifacts/analysis/figures/leakage.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View GO/NOGO report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "report"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "report_path = Path('report/stage1_5_report.md')\n",
    "if report_path.exists():\n",
    "    display(Markdown(report_path.read_text()))\n",
    "else:\n",
    "    print('Report not found, ensure the pipeline ran successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. (Optional) Sync artifacts back to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sync"
   },
   "outputs": [],
   "source": [
    "# Example: copy metrics/report to Drive folder\n",
    "# !cp -r artifacts /content/drive/MyDrive/stage1_5_artifacts\n",
    "# !cp -r report /content/drive/MyDrive/stage1_5_report\n",
    "print('Sync commands commented out by default.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
