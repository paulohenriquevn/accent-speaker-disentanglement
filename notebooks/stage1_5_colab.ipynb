{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wVOT7d8pZXp"
      },
      "source": [
        "# Stage 1.5 — Colab Runner\n",
        "This notebook automates the Stage 1.5 latent separability audit (Accent × Speaker) inside Google Colab.\n",
        "\n",
        "**Pipeline overview**\n",
        "1. (Optional) Mount Google Drive to access private datasets/checkpoints.\n",
        "2. Clone this repository (or pull from your fork).\n",
        "3. Install dependencies with `pip install -e .[dev]`.\n",
        "4. Ensure `data/manifest.jsonl` and referenced audio files exist.\n",
        "5. Run the feature extractors (acoustic, ECAPA, SSL, backbone).\n",
        "6. Execute `stage1_5 run` to train probes, compute leakage/RSA/CKA, and render the GO/NOGO report.\n",
        "7. Download artifacts (`artifacts/analysis`, `report/`) or sync back to Drive."
      ],
      "id": "2wVOT7d8pZXp"
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content\n"
      ],
      "metadata": {
        "id": "inmp-d8mpqwo"
      },
      "id": "inmp-d8mpqwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "gO4MQbM5qTn3"
      },
      "id": "gO4MQbM5qTn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/stage1_5"
      ],
      "metadata": {
        "id": "9qoVLb6EqWm0"
      },
      "id": "9qoVLb6EqWm0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydzm5OUnpZXr"
      },
      "source": [
        "## 1. Runtime diagnostics"
      ],
      "id": "Ydzm5OUnpZXr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi || echo 'GPU not available (OK for CPU-only runs)'"
      ],
      "id": "gpu-check"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKj0IZAxpZXs"
      },
      "source": [
        "## 2. (Optional) Mount Google Drive\n",
        "If your dataset or checkpoints live on Drive, mount it now. Skip if you plan to upload files manually."
      ],
      "id": "lKj0IZAxpZXs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "MOUNT_DRIVE = False  # set to True if you want to mount Drive\n",
        "if MOUNT_DRIVE:\n",
        "    drive.mount('/content/drive')"
      ],
      "id": "mount-drive"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngUwzuyFpZXs"
      },
      "source": [
        "## 3. Repository + dataset configuration\n",
        "Set the repository URL/branch you want to run. Update paths if your manifest or audio live elsewhere (e.g., in Drive)."
      ],
      "id": "ngUwzuyFpZXs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = 'https://github.com/paulohenriquevn/accent-speaker-disentanglement.git'  # TODO: update\n",
        "BRANCH = 'main'                                        # e.g., 'main' or 'stage1_5'\n",
        "WORKDIR = Path('/content/stage1_5')\n",
        "DATA_ROOT = WORKDIR / 'data'                           # adjust if mounting from Drive\n",
        "MANIFEST_PATH = DATA_ROOT / 'manifest.jsonl'\n",
        "CONFIG_PATH = Path('config/stage1_5.yaml')             # relative to WORKDIR\n",
        "\n",
        "print('Repo:', REPO_URL)\n",
        "print('Branch:', BRANCH)\n",
        "print('Working dir:', WORKDIR)"
      ],
      "id": "config"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPC-FLgmpZXs"
      },
      "source": [
        "## 4. Clone / refresh the project"
      ],
      "id": "qPC-FLgmpZXs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone"
      },
      "outputs": [],
      "source": [
        "!git clone -b $BRANCH $REPO_URL 'stage1_5'\n",
        "%cd stage1_5"
      ],
      "id": "clone"
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "37DPk57yuQYq"
      },
      "id": "37DPk57yuQYq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeT50addpZXs"
      },
      "source": [
        "## 5. Install dependencies"
      ],
      "id": "yeT50addpZXs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U pip\n",
        "!pip install -q -e .[dev]\n",
        "!pip install -q -U qwen-tts"
      ],
      "id": "install"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XoY-oH-pZXt"
      },
      "source": [
        "## 6. Dataset preparation\n",
        "Refer to `docs/dataset_guidelines.md` for the full specification.\n",
        "\n",
        "Options:\n",
        "- **Auto**: provide a dataset archive URL + metadata CSV and let the CLI download/extract/build the manifest.\n",
        "- **Manual**: upload your own `data/wav` + `data/manifest.jsonl` (skip the auto cells).\n"
      ],
      "id": "0XoY-oH-pZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-config"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATASET_URL = ''  # e.g., https://drive.google.com/uc?id=...\n",
        "DATASET_ARCHIVE_NAME = 'stage1_5_dataset.zip'\n",
        "DATASET_EXTRACT_DIR = Path('data/external')\n",
        "DATASET_METADATA_CSV = DATASET_EXTRACT_DIR / 'metadata.csv'\n",
        "DATASET_AUDIO_SUBDIR = DATASET_EXTRACT_DIR / 'wav'\n",
        "COPY_AUDIO_TO = DATA_ROOT / 'wav'\n",
        "AUTO_BUILD_MANIFEST = False  # set True to run CLI steps automatically\n",
        "print('Dataset extract dir:', DATASET_EXTRACT_DIR)\n"
      ],
      "id": "dataset-config"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-download"
      },
      "outputs": [],
      "source": [
        "if DATASET_URL:\n",
        "    !stage1_5 dataset download --url $DATASET_URL --output-dir $DATASET_EXTRACT_DIR --filename $DATASET_ARCHIVE_NAME\n",
        "else:\n",
        "    print('Set DATASET_URL to enable automatic download or skip to manual upload.')\n"
      ],
      "id": "dataset-download"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-build"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "if AUTO_BUILD_MANIFEST and DATASET_AUDIO_SUBDIR.exists():\n",
        "    COPY_AUDIO_TO.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copytree(DATASET_AUDIO_SUBDIR, COPY_AUDIO_TO, dirs_exist_ok=True)\n",
        "    if DATASET_METADATA_CSV.exists():\n",
        "        !stage1_5 dataset build-manifest $DATASET_METADATA_CSV --audio-root $COPY_AUDIO_TO --output $MANIFEST_PATH --source real\n",
        "    else:\n",
        "        raise FileNotFoundError(f'Metadata CSV not found: {DATASET_METADATA_CSV}')\n",
        "else:\n",
        "    print('AUTO_BUILD_MANIFEST disabled or dataset folders missing; ensure data/manifest.jsonl exists.')\n"
      ],
      "id": "dataset-build"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "# Manual fallback: copy dataset into data/ then ensure manifest exists\n",
        "# Example: copy dataset from Drive\n",
        "# !cp -r /content/drive/MyDrive/stage1_5_data/* $DATA_ROOT\n",
        "\n",
        "if not MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f'Manifest not found: {MANIFEST_PATH}. Provide metadata or enable AUTO_BUILD_MANIFEST.')\n",
        "\n",
        "print('Manifest entries preview:')\n",
        "!head -n 5 $MANIFEST_PATH\n"
      ],
      "id": "dataset"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y transformers\n",
        "!pip -q install \"transformers>=4.41,<4.50\"\n"
      ],
      "metadata": {
        "id": "h_2GVuBztBXA"
      },
      "id": "h_2GVuBztBXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "print(hasattr(transformers, \"AutoModel\"))"
      ],
      "metadata": {
        "id": "XLDWuPaktDRa"
      },
      "id": "XLDWuPaktDRa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixes for backbone run (synthetic manifest + texts.json + layers)\n",
        "These cells generate `data/texts.json` and `gen/manifest_syn.jsonl` required by `stage1_5 features backbone`.\n"
      ],
      "metadata": {
        "id": "q1ASH8K61S-a"
      },
      "id": "q1ASH8K61S-a"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U qwen-tts"
      ],
      "metadata": {
        "id": "LxUzJjG67NrM"
      },
      "id": "LxUzJjG67NrM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerar os textos\n"
      ],
      "metadata": {
        "id": "SXIZ9cUfE2Aj"
      },
      "id": "SXIZ9cUfE2Aj"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "python - <<'PY'\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "p = Path(\"gen/manifest_syn.jsonl\")\n",
        "assert p.exists(), f\"Arquivo não existe: {p}\"\n",
        "\n",
        "bad = 0\n",
        "for i, line in enumerate(p.read_text().splitlines(), 1):\n",
        "    try:\n",
        "        row = json.loads(line)\n",
        "        for k in [\"utt_id\",\"path\",\"speaker\",\"accent\",\"text_id\",\"source\"]:\n",
        "            assert k in row, f\"linha {i}: faltando {k}\"\n",
        "    except Exception as e:\n",
        "        bad += 1\n",
        "        print(\"ERRO na linha\", i, \"->\", e)\n",
        "\n",
        "print(\"OK\" if bad == 0 else f\"Falhas: {bad}\")\n",
        "PY\n"
      ],
      "metadata": {
        "id": "IIZkDoMVBQgM"
      },
      "id": "IIZkDoMVBQgM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure data/texts.json and data/texts.jsonl exist for backbone CLI\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "jsonl_path = DATA_ROOT / \"texts.jsonl\"\n",
        "json_path = DATA_ROOT / \"texts.json\"\n",
        "\n",
        "default_texts = [\n",
        "    \"Bom dia, obrigado por participar do experimento.\",\n",
        "    \"Hoje o tempo esta firme e o ceu esta limpo.\",\n",
        "    \"A equipe avaliou o modelo em diferentes regioes.\",\n",
        "    \"O objetivo e medir separabilidade de sotaque e identidade.\",\n",
        "    \"Leia a frase com voz neutra e ritmo constante.\",\n",
        "    \"A gravacao deve ser clara, sem ruido de fundo.\",\n",
        "    \"O cachorro correu pelo quintal com alegria.\",\n",
        "    \"A chuva começou no fim da tarde e parou cedo.\",\n",
        "    \"O professor explicou a tarefa com paciencia.\",\n",
        "    \"A menina abriu a janela para entrar ar.\",\n",
        "    \"O mercado fica aberto ate as oito da noite.\",\n",
        "    \"O time treinou para o jogo do fim de semana.\",\n",
        "    \"A cidade cresce e precisa de novos servicos.\",\n",
        "    \"O barco saiu do porto ao amanhecer.\",\n",
        "    \"O cafe estava quente e o pao estava fresco.\",\n",
        "    \"A biblioteca recebe estudantes todos os dias.\",\n",
        "    \"A musica tocava baixo durante a reuniao.\",\n",
        "    \"O medico pediu descanso e hidratacao.\",\n",
        "    \"A estrada estava livre e o caminho foi rapido.\",\n",
        "    \"O artista pintou o quadro com cores vivas.\",\n",
        "    \"A casa antiga foi reformada com cuidado.\",\n",
        "    \"O relogio marcou seis horas e a aula terminou.\",\n",
        "    \"A feira oferecia frutas maduras e doces.\",\n",
        "    \"O passageiro perdeu o onibus por pouco.\",\n",
        "    \"A noticia foi confirmada pela equipe tecnica.\",\n",
        "    \"O jardim tem flores vermelhas e amarelas.\",\n",
        "    \"A crianca contou uma historia divertida.\",\n",
        "    \"O vento soprava forte e frio naquela noite.\",\n",
        "    \"O aluno revisou o conteudo antes da prova.\",\n",
        "    \"O telefone tocou cedo e acordou a familia.\"\n",
        "]\n",
        "\n",
        "if jsonl_path.exists():\n",
        "    arr = [json.loads(l) for l in jsonl_path.read_text(encoding=\"utf-8\").splitlines() if l.strip()]\n",
        "    json_path.write_text(json.dumps(arr, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(\"Wrote\", json_path, \"items:\", len(arr))\n",
        "elif json_path.exists():\n",
        "    arr = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "    jsonl_path.write_text(\"\\n\".join(json.dumps(x, ensure_ascii=False) for x in arr) + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"Wrote\", jsonl_path, \"items:\", len(arr))\n",
        "else:\n",
        "    arr = [{\"text_id\": f\"t{i:02d}\", \"text\": t} for i, t in enumerate(default_texts)]\n",
        "    json_path.write_text(json.dumps(arr, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    jsonl_path.write_text(\"\\n\".join(json.dumps(x, ensure_ascii=False) for x in arr) + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"Generated default texts ->\", json_path, \"and\", jsonl_path, \"items:\", len(arr))"
      ],
      "metadata": {
        "id": "HPjZnD251SMA"
      },
      "id": "HPjZnD251SMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate gen/manifest_syn.jsonl with the required ManifestEntry fields\n",
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "texts_path = DATA_ROOT / \"texts.json\"\n",
        "out = Path(\"gen/manifest_syn.jsonl\")\n",
        "out.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "texts = json.loads(texts_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "ACCENTS = [\"NE\", \"SE\", \"S\"]\n",
        "SPEAKER = \"ryan\"   # keep aligned with backbone.generation.speaker\n",
        "rows = []\n",
        "\n",
        "for i, obj in enumerate(texts):\n",
        "    text_id = obj[\"text_id\"]\n",
        "    utt_id = f\"syn_{i:06d}\"\n",
        "    wav_path = f\"gen/synthetic_audio/{utt_id}.wav\"\n",
        "    rows.append({\n",
        "        \"utt_id\": utt_id,\n",
        "        \"path\": wav_path,\n",
        "        \"speaker\": SPEAKER,\n",
        "        \"accent\": random.choice(ACCENTS),\n",
        "        \"text_id\": text_id,\n",
        "        \"source\": \"synthetic\",\n",
        "    })\n",
        "\n",
        "with out.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for r in rows:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote\", out, \"rows:\", len(rows))"
      ],
      "metadata": {
        "id": "9qR9EA-91YnT"
      },
      "id": "9qR9EA-91YnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h91jFoffpZXt"
      },
      "source": [
        "## 7. Feature extraction\n",
        "Uncomment the commands you need. You may run them separately to reuse cached features."
      ],
      "id": "h91jFoffpZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "features"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "# Acoustic features PASS\n",
        "# stage1_5 features acoustic data/manifest.jsonl artifacts/features/acoustic\n",
        "\n",
        "# ECAPA embeddings (set device to 'cuda' if GPU is available) PASS\n",
        "# stage1_5 features ecapa data/manifest.jsonl artifacts/features/ecapa --device cuda\n",
        "\n",
        "# SSL features (HuBERT/WavLM via Hugging Face Transformers)\n",
        "# stage1_5 features ssl data/manifest.jsonl artifacts/features/ssl --model wavlm_large\n",
        "\n",
        "# Backbone hooks (requires synthetic manifest + text prompts)\n",
        "stage1_5 features backbone gen/manifest_syn.jsonl data/texts.json artifacts/features/backbone \\\n",
        "  \"Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice\" \\\n",
        "  --layers text_encoder_out \\\n",
        "  --layers decoder_block_04 \\\n",
        "  --layers decoder_block_08 \\\n",
        "  --layers pre_vocoder\n",
        "\n",
        "\n"
      ],
      "id": "features"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1x9Z_rRxvBhF"
      },
      "id": "1x9Z_rRxvBhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuzDOe_epZXt"
      },
      "source": [
        "## 8. Run Stage 1.5 pipeline"
      ],
      "id": "BuzDOe_epZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run"
      },
      "outputs": [],
      "source": [
        "!stage1_5 run $CONFIG_PATH"
      ],
      "id": "run"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9xICOwYpZXt"
      },
      "source": [
        "## 9. Inspect metrics & figures"
      ],
      "id": "p9xICOwYpZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "metrics = pd.read_csv('artifacts/analysis/metrics.csv')\n",
        "metrics.sort_values('accent_f1', ascending=False).head()"
      ],
      "id": "metrics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "figures"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image('artifacts/analysis/figures/accent_f1.png'))\n",
        "display(Image('artifacts/analysis/figures/leakage.png'))"
      ],
      "id": "figures"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqIQbczlpZXt"
      },
      "source": [
        "## 10. View GO/NOGO report"
      ],
      "id": "WqIQbczlpZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "report"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "report_path = Path('report/stage1_5_report.md')\n",
        "if report_path.exists():\n",
        "    display(Markdown(report_path.read_text()))\n",
        "else:\n",
        "    print('Report not found, ensure the pipeline ran successfully.')"
      ],
      "id": "report"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twAqTURnpZXt"
      },
      "source": [
        "## 11. (Optional) Sync artifacts back to Drive"
      ],
      "id": "twAqTURnpZXt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sync"
      },
      "outputs": [],
      "source": [
        "# Example: copy metrics/report to Drive folder\n",
        "# !cp -r artifacts /content/drive/MyDrive/stage1_5_artifacts\n",
        "# !cp -r report /content/drive/MyDrive/stage1_5_report\n",
        "print('Sync commands commented out by default.')"
      ],
      "id": "sync"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}