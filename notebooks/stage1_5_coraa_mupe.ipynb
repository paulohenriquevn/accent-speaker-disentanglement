{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 1.5 — CORAA-MUPE-ASR Experiment\n",
        "\n",
        "**Research question:** *Does the frozen TTS backbone already understand accent in a way that is separable from speaker identity?*\n",
        "\n",
        "This notebook runs the **complete Stage 1.5 latent separability audit** using the\n",
        "[CORAA-MUPE-ASR](https://huggingface.co/datasets/nilc-nlp/CORAA-MUPE-ASR) dataset —\n",
        "289 life story interviews (365h) of spontaneous Brazilian Portuguese speech.\n",
        "\n",
        "### Pipeline overview\n",
        "\n",
        "| Step | What happens |\n",
        "|------|-------------|\n",
        "| 1 | Environment setup (GPU check, dependencies) |\n",
        "| 2 | Download CORAA-MUPE-ASR from Hugging Face & build manifest |\n",
        "| 3 | Generate `texts.json` from `normalized_text` (for backbone extractor) |\n",
        "| 4 | Extract features: acoustic, ECAPA, SSL (WavLM), backbone (Qwen3-TTS) |\n",
        "| 5 | Update config to match CORAA regions |\n",
        "| 6 | Run probes + analysis → GO / GO_CONDITIONAL / NOGO decision |\n",
        "| 7 | Inspect results (metrics table, heatmaps, report) |\n",
        "\n",
        "### Decision criteria\n",
        "\n",
        "| Decision | Accent F1 (macro) | Leakage (a→s) | Text drop |\n",
        "|----------|-------------------|---------------|----------|\n",
        "| **GO** | ≥ 0.55 | ≤ chance + 7pp | ≤ 10pp |\n",
        "| **GO_CONDITIONAL** | ≥ 0.45 | ≤ chance + 12pp | ≤ 10pp |\n",
        "| **NOGO** | < 0.40 (all backbone & SSL) | — | — |\n",
        "\n",
        "> **Runtime estimate:** ~2–4 hours on Colab L4 GPU depending on dataset filters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we are in /content\n",
        "import os\n",
        "os.makedirs('/content', exist_ok=True)\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 GPU diagnostics\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
        "else:\n",
        "    print('WARNING: No GPU detected. Backbone extraction will be very slow.')\n",
        "\n",
        "!nvidia-smi || echo 'nvidia-smi not available'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2 Clone repository\n",
        "REPO_URL = 'https://github.com/paulohenriquevn/accent-speaker-disentanglement.git'  # TODO: update\n",
        "BRANCH = 'main'\n",
        "\n",
        "!rm -rf /content/stage1_5_repo\n",
        "!git clone -b {BRANCH} {REPO_URL} /content/stage1_5_repo\n",
        "%cd /content/stage1_5_repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.3 Install dependencies\n",
        "# NOTE: qwen-tts is needed for backbone extraction (Qwen3-TTS model)\n",
        "!pip install -q -U pip\n",
        "!pip install -q -e .[dev]\n",
        "!pip install -q -U qwen-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.4 Verify installation\n",
        "!stage1_5 --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5 (Optional) Mount Google Drive for persistent storage\n",
        "MOUNT_DRIVE = False  # Set True to mount Drive\n",
        "DRIVE_ARTIFACT_DIR = '/content/drive/MyDrive/stage1_5_coraa'  # Where to save results\n",
        "\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs(DRIVE_ARTIFACT_DIR, exist_ok=True)\n",
        "    print(f'Drive mounted. Artifacts will be synced to: {DRIVE_ARTIFACT_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Dataset preparation (CORAA-MUPE-ASR)\n",
        "\n",
        "We use the `build-coraa` CLI command which:\n",
        "1. Downloads the dataset from Hugging Face (`nilc-nlp/CORAA-MUPE-ASR`)\n",
        "2. Filters only interviewees (`speaker_type='R'`) who have `birth_state` metadata\n",
        "3. Maps 27 Brazilian states to 5 IBGE macro-regions: `N`, `NE`, `CO`, `SE`, `S`\n",
        "4. Applies configurable filters (regions, duration, quality, max samples per speaker)\n",
        "5. Exports audio and writes `manifest.jsonl`\n",
        "\n",
        "### Filter rationale\n",
        "- **Regions**: We focus on `NE`, `SE`, `S` (the three regions with the most phonetic contrast). Set to `None` to include all 5 regions.\n",
        "- **Duration**: Segments between 3–15 seconds to avoid very short/uninformative clips and very long segments that strain GPU memory during backbone extraction.\n",
        "- **Audio quality**: `high` only to reduce noise confounds.\n",
        "- **Max samples per speaker**: Cap at 30 to mitigate class imbalance and prevent speaker memorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Configurable dataset filters\n",
        "# Adjust these to control dataset size and composition.\n",
        "\n",
        "REGIONS = 'NE,SE,S'          # Comma-separated macro-region codes, or None for all 5\n",
        "MIN_DURATION = 3.0            # seconds\n",
        "MAX_DURATION = 15.0           # seconds\n",
        "AUDIO_QUALITY = 'high'        # 'high', 'low', or None for both\n",
        "MAX_SAMPLES_PER_SPEAKER = 30  # None for unlimited\n",
        "\n",
        "MANIFEST_PATH = 'data/manifest.jsonl'\n",
        "AUDIO_DIR = 'data/wav/coraa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2 Build manifest from CORAA-MUPE-ASR\n",
        "# Calls the Python function directly so tqdm progress and errors are visible.\n",
        "# First run will take several minutes for the HF download + audio export.\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s %(name)s: %(message)s', force=True)\n",
        "\n",
        "# Force-reload to pick up any git-pulled changes without restarting runtime\n",
        "import importlib, stage1_5.data.dataset_builder as _dsb\n",
        "importlib.reload(_dsb)\n",
        "from stage1_5.data.dataset_builder import build_manifest_from_coraa\n",
        "\n",
        "region_list = [r.strip() for r in REGIONS.split(',')] if REGIONS else None\n",
        "\n",
        "manifest_result = build_manifest_from_coraa(\n",
        "    output_path=MANIFEST_PATH,\n",
        "    audio_dir=AUDIO_DIR,\n",
        "    regions=region_list,\n",
        "    min_duration=MIN_DURATION,\n",
        "    max_duration=MAX_DURATION,\n",
        "    audio_quality=AUDIO_QUALITY,\n",
        "    max_samples_per_speaker=MAX_SAMPLES_PER_SPEAKER,\n",
        ")\n",
        "print(f'Manifest written to: {manifest_result}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3 Verify manifest & dataset statistics\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "manifest_path = Path(MANIFEST_PATH)\n",
        "assert manifest_path.exists(), f'Manifest not found: {manifest_path}'\n",
        "\n",
        "rows = [json.loads(line) for line in manifest_path.read_text().splitlines() if line.strip()]\n",
        "df_manifest = pd.DataFrame(rows)\n",
        "\n",
        "print(f'Total utterances: {len(df_manifest)}')\n",
        "print(f'Unique speakers:  {df_manifest[\"speaker\"].nunique()}')\n",
        "print(f'Unique accents:   {df_manifest[\"accent\"].nunique()}')\n",
        "print(f'Unique text_ids:  {df_manifest[\"text_id\"].nunique()}')\n",
        "print()\n",
        "print('Utterances per accent (region):')\n",
        "print(df_manifest['accent'].value_counts().to_string())\n",
        "print()\n",
        "print('Speakers per accent:')\n",
        "print(df_manifest.groupby('accent')['speaker'].nunique().to_string())\n",
        "print()\n",
        "print('Sample manifest entry:')\n",
        "print(json.dumps(rows[0], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.4 FULL validation: check ALL audio files exist before feature extraction\n",
        "# This catches export failures early, before spending hours on extraction.\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(f'Current working directory: {os.getcwd()}')\n",
        "print(f'Checking {len(rows)} audio paths from manifest...\\n')\n",
        "\n",
        "missing = []\n",
        "exists_count = 0\n",
        "total_bytes = 0\n",
        "for r in rows:\n",
        "    p = Path(r['path'])\n",
        "    if p.exists():\n",
        "        exists_count += 1\n",
        "        total_bytes += p.stat().st_size\n",
        "    else:\n",
        "        missing.append(str(p))\n",
        "\n",
        "print(f'Audio files found:   {exists_count} / {len(rows)}')\n",
        "print(f'Audio files missing: {len(missing)} / {len(rows)}')\n",
        "print(f'Total audio size:    {total_bytes / (1024**3):.2f} GB')\n",
        "\n",
        "if missing:\n",
        "    print(f'\\nFirst 10 missing paths:')\n",
        "    for mp in missing[:10]:\n",
        "        print(f'  {mp}')\n",
        "    # Check if the paths are relative and CWD might be wrong\n",
        "    if not Path(missing[0]).is_absolute():\n",
        "        print(f'\\nPaths are RELATIVE. CWD is: {os.getcwd()}')\n",
        "        print(f'Expected absolute paths after the fix. Re-run build-coraa.')\n",
        "    raise RuntimeError(\n",
        "        f'{len(missing)} audio files are missing! '\n",
        "        f'Feature extraction will fail. Fix the dataset build step first.'\n",
        "    )\n",
        "else:\n",
        "    print('\\nAll audio files verified. Ready for feature extraction.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Generate `texts.json` for backbone extraction\n",
        "\n",
        "The backbone feature extractor (Qwen3-TTS) requires a text prompt for each utterance.\n",
        "Since CORAA-MUPE is spontaneous speech (not controlled reading), we use the `normalized_text`\n",
        "from the dataset as the text prompt. The `text_id` field links each manifest entry to its text.\n",
        "\n",
        "We need to:\n",
        "1. Reload the CORAA-MUPE dataset to access `normalized_text` (the manifest only has `text_id`)\n",
        "2. Build a mapping `{text_id: normalized_text}` for all entries in the manifest\n",
        "3. Write `data/texts.json` in the format `[{\"text_id\": ..., \"text\": ...}, ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Build texts.json from CORAA-MUPE normalized_text\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reload manifest (self-contained — no dependency on earlier cell variables)\n",
        "manifest_path = Path(MANIFEST_PATH)\n",
        "assert manifest_path.exists(), f'Manifest not found: {manifest_path}. Run step 2 first.'\n",
        "rows = [json.loads(line) for line in manifest_path.read_text().splitlines() if line.strip()]\n",
        "df_manifest = pd.DataFrame(rows)\n",
        "manifest_text_ids = set(df_manifest['text_id'].unique())\n",
        "\n",
        "# Load the HF dataset (uses cache from step 2)\n",
        "print('Loading CORAA-MUPE-ASR (from cache)...')\n",
        "ds = load_dataset('nilc-nlp/CORAA-MUPE-ASR', split='train')\n",
        "# Exclude audio column to avoid slow/broken serialisation\n",
        "metadata_cols = [c for c in ds.column_names if c != 'audio']\n",
        "coraa_df = ds.select_columns(metadata_cols).to_pandas()\n",
        "\n",
        "# Build text_id the same way as build_manifest_from_coraa()\n",
        "coraa_df_r = coraa_df[coraa_df['speaker_type'] == 'R'].copy()\n",
        "coraa_df_r['text_id_computed'] = (\n",
        "    coraa_df_r['audio_name'].astype(str) + '_' +\n",
        "    coraa_df_r['start_time'].astype(str).str.replace('.', '_', regex=False)\n",
        ")\n",
        "\n",
        "# Build the texts list\n",
        "texts_list = []\n",
        "seen = set()\n",
        "for _, row in coraa_df_r.iterrows():\n",
        "    tid = row['text_id_computed']\n",
        "    if tid in manifest_text_ids and tid not in seen:\n",
        "        text = str(row.get('normalized_text', '') or row.get('original_text', ''))\n",
        "        if text.strip():\n",
        "            texts_list.append({'text_id': tid, 'text': text.strip()})\n",
        "            seen.add(tid)\n",
        "\n",
        "# Write texts.json\n",
        "texts_json_path = Path('data/texts.json')\n",
        "texts_json_path.write_text(json.dumps(texts_list, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "print(f'Wrote {len(texts_list)} text entries to {texts_json_path}')\n",
        "print(f'Manifest text_ids: {len(manifest_text_ids)}')\n",
        "missing = manifest_text_ids - seen\n",
        "if missing:\n",
        "    print(f'WARNING: {len(missing)} text_ids in manifest have no text. Backbone extraction will skip them.')\n",
        "    print(f'  Examples: {list(missing)[:5]}')\n",
        "else:\n",
        "    print('All manifest text_ids have corresponding texts.')\n",
        "\n",
        "print(f'\\nSample entry: {texts_list[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Feature extraction\n",
        "\n",
        "We extract 4 types of features, each producing `.npz` files per utterance:\n",
        "\n",
        "| Extractor | Dimensionality | Purpose |\n",
        "|-----------|---------------|--------|\n",
        "| **Acoustic** | 85-dim (MFCC mean/std + F0 stats + speaking rate + duration) | Baseline: surface-level accent cues |\n",
        "| **ECAPA** | 192-dim (SpeechBrain speaker embeddings) | Control: speaker identity |\n",
        "| **SSL (WavLM-large)** | 1024-dim per layer (layers 0,6,12,18,24) | Baseline: self-supervised speech representations |\n",
        "| **Backbone (Qwen3-TTS)** | Variable-dim per layer | Test: does the TTS backbone encode accent? |\n",
        "\n",
        "Each extractor can be run independently. Cached features are reused if already present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Acoustic features (~1-3 min)\n",
        "# MFCC mean/std (40+40), F0 stats (3), speaking rate (1), duration (1) = 85 dimensions\n",
        "!stage1_5 features acoustic {MANIFEST_PATH} artifacts/features/acoustic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 ECAPA speaker embeddings (~5-10 min on GPU)\n",
        "# 192-dim embeddings from SpeechBrain's pretrained ECAPA-TDNN\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "!stage1_5 features ecapa {MANIFEST_PATH} artifacts/features/ecapa --device {DEVICE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.3 SSL features - WavLM-large (~10-20 min on GPU)\n",
        "# 1024-dim per layer, layers [0, 6, 12, 18, 24]\n",
        "!stage1_5 features ssl \\\n",
        "    {MANIFEST_PATH} \\\n",
        "    artifacts/features/ssl \\\n",
        "    --model wavlm_large \\\n",
        "    --device {DEVICE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.4 Backbone features - Qwen3-TTS (~30-60 min on GPU)\n",
        "# Hooks into internal layers: text_encoder_out, decoder_block_04, decoder_block_08, pre_vocoder\n",
        "# Requires texts.json (built in step 3)\n",
        "!stage1_5 features backbone \\\n",
        "    {MANIFEST_PATH} \\\n",
        "    data/texts.json \\\n",
        "    artifacts/features/backbone \\\n",
        "    \"Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice\" \\\n",
        "    --layers text_encoder_out decoder_block_04 decoder_block_08 pre_vocoder \\\n",
        "    --device {DEVICE} \\\n",
        "    --dtype bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.5 Verify extracted features\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "feature_dirs = {\n",
        "    'acoustic': Path('artifacts/features/acoustic'),\n",
        "    'ecapa': Path('artifacts/features/ecapa'),\n",
        "    'ssl': Path('artifacts/features/ssl'),\n",
        "    'backbone': Path('artifacts/features/backbone'),\n",
        "}\n",
        "\n",
        "for name, fdir in feature_dirs.items():\n",
        "    npz_files = list(fdir.glob('*.npz'))\n",
        "    if not npz_files:\n",
        "        print(f'  {name}: NO FILES FOUND')\n",
        "        continue\n",
        "    sample = np.load(npz_files[0])\n",
        "    keys = list(sample.files)\n",
        "    dims = {k: sample[k].shape for k in keys}\n",
        "    print(f'  {name}: {len(npz_files)} files, keys={keys}, dims={dims}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Update experiment config for CORAA-MUPE\n",
        "\n",
        "The default `config/stage1_5.yaml` uses `accents: [\"NE\", \"SE\", \"S\"]`. We update it\n",
        "to match the regions actually present in our manifest. The config drives the `stage1_5 run`\n",
        "pipeline which trains probes and computes the GO/NOGO decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 Patch config to match actual dataset\n",
        "import yaml\n",
        "\n",
        "config_path = Path('config/stage1_5.yaml')\n",
        "cfg = yaml.safe_load(config_path.read_text())\n",
        "\n",
        "# Update accents to match what's actually in the manifest\n",
        "actual_accents = sorted(df_manifest['accent'].unique().tolist())\n",
        "cfg['experiment']['accents'] = actual_accents\n",
        "print(f'Accents in manifest: {actual_accents}')\n",
        "\n",
        "# Ensure paths are correct\n",
        "cfg['paths']['manifest'] = MANIFEST_PATH\n",
        "\n",
        "# Write updated config\n",
        "config_path.write_text(yaml.dump(cfg, default_flow_style=False, allow_unicode=True))\n",
        "print(f'Config updated: {config_path}')\n",
        "print(f'\\nExperiment thresholds:')\n",
        "print(f'  GO:          F1 >= {cfg[\"experiment\"][\"min_f1_go\"]}, leakage <= chance + {cfg[\"experiment\"][\"leakage_margin_pp\"]}pp')\n",
        "print(f'  CONDITIONAL: F1 >= {cfg[\"experiment\"][\"min_f1_conditional\"]}, leakage <= chance + {cfg[\"experiment\"][\"leakage_conditional_margin_pp\"]}pp')\n",
        "print(f'  Text drop tolerance: {cfg[\"experiment\"][\"text_drop_tolerance_pp\"]}pp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Run the full pipeline\n",
        "\n",
        "The `stage1_5 run` command:\n",
        "1. Loads the manifest and all extracted features\n",
        "2. For each feature space, trains logistic regression probes with:\n",
        "   - **Speaker-disjoint splits** for accent classification (no speaker seen in both train & test)\n",
        "   - **Text-disjoint splits** for robustness testing\n",
        "   - **Stratified splits** for speaker classification\n",
        "3. Computes: accent F1 (macro), speaker accuracy, leakage (a→s, s→a), text-drop, RSA, CKA\n",
        "4. Applies decision logic (GO / GO_CONDITIONAL / NOGO)\n",
        "5. Generates heatmap figures and a markdown report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1 Run the pipeline\n",
        "!stage1_5 run config/stage1_5.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 Load metrics\n",
        "import pandas as pd\n",
        "\n",
        "metrics_path = Path('artifacts/analysis/metrics.csv')\n",
        "assert metrics_path.exists(), f'Metrics not found: {metrics_path}. Did the pipeline run successfully?'\n",
        "\n",
        "metrics = pd.read_csv(metrics_path)\n",
        "print(f'Total feature spaces evaluated: {len(metrics)}')\n",
        "print()\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2 Key metrics: accent separability (sorted by F1)\n",
        "accent_cols = ['label', 'target', 'accent_f1', 'accent_text_f1', 'accent_text_drop',\n",
        "               'leakage_a2s', 'chance_speaker', 'rsa_accent', 'cka_accent']\n",
        "available_cols = [c for c in accent_cols if c in metrics.columns]\n",
        "print('Accent separability (sorted by F1, descending):')\n",
        "metrics[available_cols].sort_values('accent_f1', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3 Leakage analysis\n",
        "# leakage_a2s = speaker accuracy when probe is trained on accent features\n",
        "# Should be close to chance_speaker for clean accent encoding\n",
        "leak_cols = ['label', 'accent_f1', 'leakage_a2s', 'chance_speaker', 'speaker_acc']\n",
        "available_leak = [c for c in leak_cols if c in metrics.columns]\n",
        "\n",
        "metrics_sorted = metrics[available_leak].sort_values('accent_f1', ascending=False)\n",
        "\n",
        "if 'leakage_a2s' in metrics.columns and 'chance_speaker' in metrics.columns:\n",
        "    metrics_sorted = metrics_sorted.copy()\n",
        "    metrics_sorted['leakage_excess_pp'] = (\n",
        "        (metrics_sorted['leakage_a2s'] - metrics_sorted['chance_speaker']) * 100\n",
        "    ).round(1)\n",
        "\n",
        "print('Leakage analysis:')\n",
        "metrics_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.4 Display heatmaps\n",
        "from IPython.display import Image, display\n",
        "\n",
        "fig_dir = Path('artifacts/analysis/figures')\n",
        "\n",
        "for fig_name in ['accent_f1.png', 'leakage.png', 'accent_text_robustness.png']:\n",
        "    fig_path = fig_dir / fig_name\n",
        "    if fig_path.exists():\n",
        "        print(f'\\n--- {fig_name} ---')\n",
        "        display(Image(str(fig_path)))\n",
        "    else:\n",
        "        print(f'Figure not found: {fig_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.5 Custom visualization: backbone layers vs SSL layers comparison\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "backbone_rows = metrics[metrics['label'].str.startswith('backbone:')].copy()\n",
        "ssl_rows = metrics[metrics['label'].str.startswith('ssl:')].copy()\n",
        "\n",
        "if not backbone_rows.empty or not ssl_rows.empty:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Accent F1 comparison\n",
        "    ax = axes[0]\n",
        "    if not backbone_rows.empty:\n",
        "        ax.barh(backbone_rows['label'], backbone_rows['accent_f1'], color='steelblue', label='Backbone')\n",
        "    if not ssl_rows.empty:\n",
        "        ax.barh(ssl_rows['label'], ssl_rows['accent_f1'], color='coral', label='SSL')\n",
        "    ax.axvline(x=0.55, color='green', linestyle='--', label='GO threshold')\n",
        "    ax.axvline(x=0.45, color='orange', linestyle='--', label='GO_COND threshold')\n",
        "    ax.axvline(x=0.40, color='red', linestyle='--', label='NOGO threshold')\n",
        "    ax.set_xlabel('Accent F1 (macro)')\n",
        "    ax.set_title('Accent Separability by Layer')\n",
        "    ax.legend(loc='lower right', fontsize=8)\n",
        "\n",
        "    # Leakage comparison\n",
        "    ax = axes[1]\n",
        "    if not backbone_rows.empty:\n",
        "        ax.barh(backbone_rows['label'], backbone_rows['leakage_a2s'], color='steelblue', label='Backbone')\n",
        "    if not ssl_rows.empty:\n",
        "        ax.barh(ssl_rows['label'], ssl_rows['leakage_a2s'], color='coral', label='SSL')\n",
        "    if 'chance_speaker' in metrics.columns:\n",
        "        chance = metrics['chance_speaker'].iloc[0]\n",
        "        ax.axvline(x=chance, color='gray', linestyle=':', label=f'Chance ({chance:.3f})')\n",
        "        ax.axvline(x=chance + 0.07, color='green', linestyle='--', label=f'GO limit (+7pp)')\n",
        "        ax.axvline(x=chance + 0.12, color='orange', linestyle='--', label=f'COND limit (+12pp)')\n",
        "    ax.set_xlabel('Leakage a→s (speaker accuracy)')\n",
        "    ax.set_title('Speaker Leakage by Layer')\n",
        "    ax.legend(loc='lower right', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('artifacts/analysis/figures/layer_comparison.png', dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No backbone or SSL results to plot.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.6 Display the GO/NOGO report\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "report_path = Path('report/stage1_5_report.md')\n",
        "if report_path.exists():\n",
        "    display(Markdown(report_path.read_text()))\n",
        "else:\n",
        "    print('Report not found. Check pipeline output above for errors.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Interpretation guide\n",
        "\n",
        "### How to read the results\n",
        "\n",
        "| Metric | What it measures | Good value |\n",
        "|--------|-----------------|------------|\n",
        "| `accent_f1` | Can a linear probe classify accent from this feature space? | Higher = more accent info (≥0.55 for GO) |\n",
        "| `leakage_a2s` | Does accent feature space leak speaker identity? | Close to `chance_speaker` = clean separation |\n",
        "| `accent_text_f1` | Accent F1 with text-disjoint split | Close to `accent_f1` = not memorizing text |\n",
        "| `accent_text_drop` | `accent_f1 - accent_text_f1` | ≤0.10 = robust to text variation |\n",
        "| `rsa_accent` | Representational Similarity Analysis correlation with accent labels | Higher = accent structure |\n",
        "| `cka_accent` | Centered Kernel Alignment with accent labels | Higher = accent structure |\n",
        "\n",
        "### What the decision means\n",
        "\n",
        "- **GO**: The backbone already encodes accent separably from speaker. Proceed to Stage 2 (LoRA training) with the identified best layer.\n",
        "- **GO_CONDITIONAL**: Weak but promising signal. Stage 2 may work but with higher risk. Consider adjusting LoRA strategy.\n",
        "- **NOGO**: No accent signal in backbone. LoRA fine-tuning on this backbone is unlikely to achieve accent control.\n",
        "\n",
        "### Important caveats for CORAA-MUPE\n",
        "\n",
        "1. **Spontaneous speech**: Unlike controlled reading, spontaneous speech has high intra-speaker variability. This makes accent classification harder but the experiment more ecologically valid.\n",
        "2. **Text-disjoint evaluation**: Since each utterance has unique text, `accent_text_f1` tests generalization to unseen text content. A large `accent_text_drop` suggests the probe memorizes text rather than accent.\n",
        "3. **Region vs accent**: We use IBGE macro-regions as accent proxy. Within-region variation exists (e.g., Minas Gerais vs São Paulo within SE), so perfect F1 is not expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. (Optional) Save artifacts to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9.1 Sync important artifacts to Drive\n",
        "if MOUNT_DRIVE:\n",
        "    import shutil\n",
        "    # Copy analysis results\n",
        "    for src in ['artifacts/analysis', 'report']:\n",
        "        dst = os.path.join(DRIVE_ARTIFACT_DIR, src)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "            print(f'Copied {src} -> {dst}')\n",
        "    # Copy manifest and config\n",
        "    for f in [MANIFEST_PATH, 'config/stage1_5.yaml', 'data/texts.json']:\n",
        "        if os.path.exists(f):\n",
        "            dst = os.path.join(DRIVE_ARTIFACT_DIR, f)\n",
        "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "            shutil.copy2(f, dst)\n",
        "            print(f'Copied {f} -> {dst}')\n",
        "    print(f'\\nAll artifacts saved to: {DRIVE_ARTIFACT_DIR}')\n",
        "else:\n",
        "    print('Drive not mounted. Download artifacts manually from the Colab file browser.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Summary & next steps\n",
        "\n",
        "After running this notebook, you should have:\n",
        "\n",
        "1. A `data/manifest.jsonl` with CORAA-MUPE utterances (filtered by region/duration/quality)\n",
        "2. Feature files in `artifacts/features/{acoustic,ecapa,ssl,backbone}/`\n",
        "3. Metrics in `artifacts/analysis/metrics.csv` and heatmaps in `artifacts/analysis/figures/`\n",
        "4. A GO/NOGO report in `report/stage1_5_report.md`\n",
        "\n",
        "**If GO or GO_CONDITIONAL:** proceed to Stage 2 (LoRA training) using the best backbone layer identified.\n",
        "\n",
        "**If NOGO:** consider:\n",
        "- Relaxing filters (more data, all 5 regions) and re-running\n",
        "- Trying a different TTS backbone\n",
        "- Investigating whether the accent signal exists but is non-linearly encoded (beyond what a logistic probe captures)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
